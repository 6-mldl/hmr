# ì•¼êµ¬ íƒ€ì 3D ë³µì› í”„ë¡œì íŠ¸ êµ¬í˜„ ê³„íšì„œ

## ğŸ“Š ì „ì²´ êµ¬í˜„ ë¡œë“œë§µ

### Phase 0: í™˜ê²½ ì¤€ë¹„ (1ì£¼)
- [x] ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] HMR í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ì‚¬ìš©ì ì‹¤í–‰: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
- [ ] ì‚¬ìš©ì ì‹¤í–‰: SMPL ëª¨ë¸ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
- [ ] ì‚¬ìš©ì ì‹¤í–‰: MPII, H36M ì—°êµ¬ì ì‹ ì²­

### Phase 1: ê¸°ë³¸ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ (1ì£¼)
- [ ] HMR ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡  í…ŒìŠ¤íŠ¸
- [ ] ì‚¬ëŒ ê²€ì¶œê¸° ì—°ë™ (YOLOX)
- [ ] í”„ë ˆì„ ë‹¨ìœ„ ë¹„ë””ì˜¤ ì²˜ë¦¬
- [ ] 3D ì‹œê°í™” (Open3D/PyRender)

### Phase 2: ì•¼êµ¬ íŠ¹í™” ê¸°ëŠ¥ (2ì£¼)
- [ ] ë°°íŠ¸ ê²€ì¶œ ë° ì¶”ì 
- [ ] ì‹œê°„ì  ìŠ¤ë¬´ë”© (Temporal Smoothing)
- [ ] ë°°íŠ¸ 3D ëª¨ë¸ë§
- [ ] ê´€ì ˆ ê°ë„/ì†ë„ ê³„ì‚°

### Phase 3: ë¶„ì„ ë° íŒì • (1ì£¼)
- [ ] ìŠ¤ìœ™ ë¶„ì„ ë¡œì§
- [ ] ë°˜ì¹™ íŒì • ê·œì¹™
- [ ] ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

---

## ğŸ”§ êµ¬í˜„ ìƒì„¸ ê³„íš

### Module 1: HMR ê¸°ë³¸ ì¶”ë¡  (ì¦‰ì‹œ ì œê³µ ê°€ëŠ¥)

#### 1-1. ë‹¨ì¼ ì´ë¯¸ì§€ ì¶”ë¡  ë˜í¼
```python
import torch
import cv2
import numpy as np
from models import hmr

class HMRInference:
    def __init__(self, model_path, smpl_path):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = hmr.HMR().to(self.device)
        self.model.load_state_dict(torch.load(model_path))
        self.model.eval()
        
    def preprocess(self, img_path, bbox):
        """ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° í¬ë¡­"""
        img = cv2.imread(img_path)
        x1, y1, x2, y2 = bbox
        crop = img[y1:y2, x1:x2]
        crop = cv2.resize(crop, (224, 224))
        crop = crop.astype(np.float32) / 255.0
        crop = torch.from_numpy(crop).permute(2, 0, 1).unsqueeze(0)
        return crop.to(self.device)
    
    def predict(self, img_path, bbox):
        """3D í¬ì¦ˆ ì˜ˆì¸¡"""
        img_tensor = self.preprocess(img_path, bbox)
        
        with torch.no_grad():
            pred = self.model(img_tensor)
            
        return {
            'vertices': pred['vertices'][0].cpu().numpy(),  # (6890, 3)
            'joints': pred['joints3d'][0].cpu().numpy(),     # (24, 3)
            'shape': pred['shape'][0].cpu().numpy(),         # (10,)
            'pose': pred['pose'][0].cpu().numpy()            # (72,)
        }

# ì‚¬ìš© ì˜ˆì‹œ
hmr_model = HMRInference('models/hmr_model.pt', 'models/smpl_neutral.pkl')
result = hmr_model.predict('image.jpg', bbox=[100, 50, 300, 500])
```

#### 1-2. ë¹„ë””ì˜¤ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```python
import cv2
from tqdm import tqdm

class VideoProcessor:
    def __init__(self, hmr_model, detector):
        self.hmr = hmr_model
        self.detector = detector
        
    def process_video(self, video_path, output_path):
        """ë¹„ë””ì˜¤ í”„ë ˆì„ë³„ ì²˜ë¦¬"""
        cap = cv2.VideoCapture(video_path)
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        results = []
        
        for frame_idx in tqdm(range(total_frames)):
            ret, frame = cap.read()
            if not ret:
                break
                
            # 1. ì‚¬ëŒ ê²€ì¶œ
            detections = self.detector.detect(frame)
            
            # 2. ê° ì‚¬ëŒì— ëŒ€í•´ HMR ì‹¤í–‰
            for det in detections:
                if det['class'] == 'person':
                    bbox = det['bbox']
                    # ì„ì‹œ ì €ì¥
                    cv2.imwrite(f'temp_frame_{frame_idx}.jpg', frame)
                    
                    # HMR ì¶”ë¡ 
                    pred = self.hmr.predict(f'temp_frame_{frame_idx}.jpg', bbox)
                    pred['frame_idx'] = frame_idx
                    pred['bbox'] = bbox
                    results.append(pred)
                    
        cap.release()
        
        # ê²°ê³¼ ì €ì¥
        np.save(output_path, results)
        return results
```

---

### Module 2: ì‚¬ëŒ ê²€ì¶œê¸° ì—°ë™ (ì¦‰ì‹œ ì œê³µ ê°€ëŠ¥)

#### 2-1. YOLOX ë˜í¼
```python
import torch

class PersonDetector:
    def __init__(self, model_name='yolox-x'):
        self.model = torch.hub.load('Megvii-BaseDetection/YOLOX', model_name)
        self.model.eval()
        
    def detect(self, frame):
        """ì‚¬ëŒ ê²€ì¶œ (bbox ë°˜í™˜)"""
        outputs = self.model(frame)
        
        detections = []
        for output in outputs:
            if output is None:
                continue
                
            bboxes = output[:, :4]  # x1, y1, x2, y2
            scores = output[:, 4]
            classes = output[:, 6]
            
            # ì‚¬ëŒ í´ë˜ìŠ¤ë§Œ í•„í„°ë§ (COCO class 0)
            person_mask = classes == 0
            
            for bbox, score in zip(bboxes[person_mask], scores[person_mask]):
                if score > 0.5:
                    detections.append({
                        'class': 'person',
                        'bbox': bbox.cpu().numpy().astype(int).tolist(),
                        'confidence': float(score)
                    })
                    
        return detections

# ì‚¬ìš© ì˜ˆì‹œ
detector = PersonDetector()
detections = detector.detect(frame)
```

---

### Module 3: 3D ì‹œê°í™” (ì¦‰ì‹œ ì œê³µ ê°€ëŠ¥)

#### 3-1. Open3D ê¸°ë°˜ ë Œë”ë§
```python
import open3d as o3d
import numpy as np

class MeshVisualizer:
    def __init__(self, smpl_faces):
        self.faces = smpl_faces
        
    def create_mesh(self, vertices):
        """SMPL verticesë¥¼ Open3D meshë¡œ ë³€í™˜"""
        mesh = o3d.geometry.TriangleMesh()
        mesh.vertices = o3d.utility.Vector3dVector(vertices)
        mesh.triangles = o3d.utility.Vector3iVector(self.faces)
        mesh.compute_vertex_normals()
        
        # ìƒ‰ìƒ ì¶”ê°€
        mesh.paint_uniform_color([0.7, 0.7, 0.9])
        return mesh
    
    def visualize(self, vertices):
        """3D ë©”ì‰¬ ì‹œê°í™”"""
        mesh = self.create_mesh(vertices)
        o3d.visualization.draw_geometries([mesh])
        
    def save_mesh(self, vertices, output_path):
        """OBJ íŒŒì¼ë¡œ ì €ì¥"""
        mesh = self.create_mesh(vertices)
        o3d.io.write_triangle_mesh(output_path, mesh)

# ì‚¬ìš© ì˜ˆì‹œ
visualizer = MeshVisualizer(smpl_faces)
visualizer.visualize(result['vertices'])
visualizer.save_mesh(result['vertices'], 'output.obj')
```

---

### Module 4: ì‹œê°„ì  ìŠ¤ë¬´ë”© (ì¦‰ì‹œ ì œê³µ ê°€ëŠ¥)

#### 4-1. 1D Gaussian Filter
```python
from scipy.ndimage import gaussian_filter1d

class TemporalSmoother:
    def __init__(self, sigma=2.0):
        self.sigma = sigma
        
    def smooth_sequence(self, joints_sequence):
        """
        joints_sequence: (T, 24, 3) - Tí”„ë ˆì„, 24ê´€ì ˆ, xyz
        """
        T, J, D = joints_sequence.shape
        smoothed = np.zeros_like(joints_sequence)
        
        for j in range(J):
            for d in range(D):
                smoothed[:, j, d] = gaussian_filter1d(
                    joints_sequence[:, j, d], 
                    sigma=self.sigma
                )
                
        return smoothed

# ì‚¬ìš© ì˜ˆì‹œ
smoother = TemporalSmoother(sigma=2.0)
joints_seq = np.array([r['joints'] for r in results])  # (T, 24, 3)
smoothed_joints = smoother.smooth_sequence(joints_seq)
```

---

### Module 5: ìš´ë™í•™ ë¶„ì„ (ì¦‰ì‹œ ì œê³µ ê°€ëŠ¥)

#### 5-1. ê´€ì ˆ ê°ì†ë„ ê³„ì‚°
```python
import numpy as np

class KinematicsAnalyzer:
    def __init__(self, fps=30):
        self.fps = fps
        self.dt = 1.0 / fps
        
    def compute_velocity(self, positions):
        """ìœ„ì¹˜ ì‹œí€€ìŠ¤ â†’ ì†ë„"""
        velocities = np.gradient(positions, axis=0) / self.dt
        return velocities
    
    def compute_acceleration(self, velocities):
        """ì†ë„ ì‹œí€€ìŠ¤ â†’ ê°€ì†ë„"""
        accelerations = np.gradient(velocities, axis=0) / self.dt
        return accelerations
    
    def compute_joint_angle(self, j1, j2, j3):
        """3ê°œ ê´€ì ˆë¡œ ê°ë„ ê³„ì‚° (j2ê°€ êº¾ì´ëŠ” ì )"""
        v1 = j1 - j2
        v2 = j3 - j2
        
        cos_angle = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))
        angle_rad = np.arccos(np.clip(cos_angle, -1.0, 1.0))
        return np.degrees(angle_rad)
    
    def analyze_swing(self, joints_sequence):
        """ìŠ¤ìœ™ ë¶„ì„"""
        # ì†ëª© ì†ë„ (ë°°íŠ¸ ì†ë„ ê·¼ì‚¬)
        wrist_idx = 21  # SMPL ì†ëª© ì¸ë±ìŠ¤
        wrist_positions = joints_sequence[:, wrist_idx, :]
        wrist_velocities = self.compute_velocity(wrist_positions)
        wrist_speeds = np.linalg.norm(wrist_velocities, axis=1)
        
        # ìµœëŒ€ ìŠ¤ìœ™ ì†ë„
        max_speed = np.max(wrist_speeds)
        max_speed_frame = np.argmax(wrist_speeds)
        
        # íŒ”ê¿ˆì¹˜ ê°ë„ (í”„ë ˆì„ë³„)
        shoulder_idx = 17
        elbow_idx = 19
        elbow_angles = []
        
        for frame in joints_sequence:
            angle = self.compute_joint_angle(
                frame[shoulder_idx],
                frame[elbow_idx],
                frame[wrist_idx]
            )
            elbow_angles.append(angle)
            
        return {
            'max_swing_speed_ms': max_speed,
            'max_swing_speed_mph': max_speed * 2.237,  # m/s -> mph
            'impact_frame_estimate': max_speed_frame,
            'elbow_angles': elbow_angles
        }

# ì‚¬ìš© ì˜ˆì‹œ
analyzer = KinematicsAnalyzer(fps=30)
swing_analysis = analyzer.analyze_swing(smoothed_joints)
print(f"Max Swing Speed: {swing_analysis['max_swing_speed_mph']:.2f} mph")
```

---

## âœ… ì œê°€ ë°”ë¡œ ì œê³µ ê°€ëŠ¥í•œ ê²ƒë“¤

### 1. ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼ë“¤
- [x] `download_datasets.ps1` - ë°ì´í„°ì…‹ ìë™ ë‹¤ìš´ë¡œë“œ
- [x] `setup_hmr.sh` - HMR í™˜ê²½ ì„¤ì •
- [x] ìœ„ì˜ ëª¨ë“  Python ëª¨ë“ˆ ì½”ë“œ (ë³µì‚¬ ê°€ëŠ¥)

### 2. í†µí•© ì‹¤í–‰ íŒŒì¼ (ë©”ì¸ íŒŒì´í”„ë¼ì¸)
```python
# main_pipeline.py - ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

import argparse
from hmr_inference import HMRInference
from person_detector import PersonDetector
from video_processor import VideoProcessor
from mesh_visualizer import MeshVisualizer
from temporal_smoother import TemporalSmoother
from kinematics_analyzer import KinematicsAnalyzer

def main(args):
    # 1. ëª¨ë¸ ì´ˆê¸°í™”
    print("Loading models...")
    detector = PersonDetector()
    hmr_model = HMRInference(args.hmr_model, args.smpl_model)
    
    # 2. ë¹„ë””ì˜¤ ì²˜ë¦¬
    print("Processing video...")
    processor = VideoProcessor(hmr_model, detector)
    results = processor.process_video(args.input_video, 'temp_results.npy')
    
    # 3. ì‹œê°„ì  ìŠ¤ë¬´ë”©
    print("Smoothing...")
    smoother = TemporalSmoother(sigma=2.0)
    joints_seq = np.array([r['joints'] for r in results])
    smoothed_joints = smoother.smooth_sequence(joints_seq)
    
    # 4. ìš´ë™í•™ ë¶„ì„
    print("Analyzing kinematics...")
    analyzer = KinematicsAnalyzer(fps=30)
    swing_analysis = analyzer.analyze_swing(smoothed_joints)
    
    # 5. ê²°ê³¼ ì €ì¥
    print("Saving results...")
    import json
    with open(args.output_json, 'w') as f:
        json.dump(swing_analysis, f, indent=2)
    
    # 6. 3D ì‹œê°í™”
    if args.visualize:
        visualizer = MeshVisualizer(smpl_faces)
        for i, result in enumerate(results[::10]):  # 10í”„ë ˆì„ë§ˆë‹¤
            visualizer.save_mesh(
                result['vertices'], 
                f'{args.output_dir}/frame_{i:04d}.obj'
            )
    
    print(f"ì™„ë£Œ! ê²°ê³¼: {args.output_json}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--input_video', required=True)
    parser.add_argument('--hmr_model', default='models/hmr_model.pt')
    parser.add_argument('--smpl_model', default='models/smpl_neutral.pkl')
    parser.add_argument('--output_json', default='output/analysis.json')
    parser.add_argument('--output_dir', default='output')
    parser.add_argument('--visualize', action='store_true')
    args = parser.parse_args()
    
    main(args)
```

---

## âš ï¸ ì‚¬ìš©ìê°€ ì§ì ‘ ìˆ˜í–‰í•´ì•¼ í•  ê²ƒë“¤

### 1. ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ í•„ìš” (ë¼ì´ì„ ìŠ¤ ì œì•½)

#### A. SMPL ëª¨ë¸
- **ì‚¬ì´íŠ¸**: https://smpl.is.tue.mpg.de/
- **ì ˆì°¨**:
  1. íšŒì›ê°€ì… (ì´ë©”ì¼ ì¸ì¦)
  2. "Downloads" í˜ì´ì§€
  3. "SMPL for Python" ë‹¤ìš´ë¡œë“œ
  4. `basicModel_neutral_lbs_10_207_0_v1.0.0.pkl` íŒŒì¼ì„ `models/` í´ë”ì— ë³µì‚¬

#### B. MPII ë°ì´í„°ì…‹ (í•™ìŠµ ì‹œ í•„ìš”)
- **ì‚¬ì´íŠ¸**: http://human-pose.mpi-inf.mpg.de/
- **ì ˆì°¨**:
  1. íšŒì›ê°€ì…
  2. `mpii_human_pose_v1.tar.gz` ë‹¤ìš´ë¡œë“œ
  3. `datasets/mpii/` í´ë”ì— ì••ì¶• í•´ì œ

#### C. Human3.6M ë°ì´í„°ì…‹ (ì„ íƒì , fine-tuning ì‹œ)
- **ì‚¬ì´íŠ¸**: http://vision.imar.ro/human3.6m/
- **ì ˆì°¨**:
  1. ì—°êµ¬ì ê³„ì • ì‹ ì²­ (ìŠ¹ì¸ 1-2ì¼)
  2. Subjects S1~S11 ë‹¤ìš´ë¡œë“œ (100GB+)
  3. `datasets/h36m/` í´ë”ì— ì €ì¥

### 2. í™˜ê²½ ì„¤ì • ì‹¤í–‰
```bash
# Linux/Mac
chmod +x setup_hmr.sh
./setup_hmr.sh

# Windows (PowerShell)
.\download_datasets.ps1
```

### 3. ì•¼êµ¬ ì˜ìƒ ì¤€ë¹„
- íƒ€ì ì˜ìƒ ìˆ˜ì§‘ (ì§ì ‘ ì´¬ì˜ or ìœ íŠœë¸Œ)
- ê¶Œì¥ ì‚¬ì–‘:
  - í•´ìƒë„: 1080p ì´ìƒ
  - FPS: 30 ì´ìƒ
  - íƒ€ì ì „ì‹  í¬í•¨
  - ë°°ê²½ ë‹¨ìˆœí• ìˆ˜ë¡ ì¢‹ìŒ

### 4. ë°°íŠ¸/ê³µ ë¼ë²¨ë§ (YOLOX Fine-tuningìš©)
- CVAT ë“±ìœ¼ë¡œ 50~100ê°œ í”„ë ˆì„ ë¼ë²¨ë§
- í´ë˜ìŠ¤: `batter`, `bat`, `ball`
- Export: YOLO format

---

## ğŸ“¦ ìµœì¢… í´ë” êµ¬ì¡°

```
baseball_3d_analysis/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ coco/
â”‚   â”œâ”€â”€ mpii/
â”‚   â”œâ”€â”€ up-3d/
â”‚   â””â”€â”€ h36m/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ hmr_model.pt
â”‚   â”œâ”€â”€ smpl_neutral.pkl
â”‚   â””â”€â”€ yolox_x.pth
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ hmr_inference.py
â”‚   â”œâ”€â”€ person_detector.py
â”‚   â”œâ”€â”€ video_processor.py
â”‚   â”œâ”€â”€ mesh_visualizer.py
â”‚   â”œâ”€â”€ temporal_smoother.py
â”‚   â”œâ”€â”€ kinematics_analyzer.py
â”‚   â””â”€â”€ main_pipeline.py
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ analysis.json
â”‚   â””â”€â”€ meshes/
â”œâ”€â”€ download_datasets.ps1
â”œâ”€â”€ setup_hmr.sh
â””â”€â”€ README.md
```

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ ìš”ì•½

### ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥ (ì œê³µëœ íŒŒì¼ë“¤)
1. `download_datasets.ps1` ì‹¤í–‰ â†’ COCO, UP-3D ìë™ ë‹¤ìš´ë¡œë“œ
2. ìœ„ì˜ Python ì½”ë“œë“¤ì„ `src/` í´ë”ì— ë³µì‚¬

### ì‚¬ìš©ì ìˆ˜ë™ ì‘ì—…
3. SMPL ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
4. MPII íšŒì›ê°€ì… ë° ë‹¤ìš´ë¡œë“œ (ì„ íƒ)
5. Human3.6M ì‹ ì²­ (ì„ íƒ)
6. ì•¼êµ¬ ì˜ìƒ ì¤€ë¹„
7. (ì„ íƒ) ë°°íŠ¸/ê³µ ë¼ë²¨ë§

### ì‹¤í–‰
8. `setup_hmr.sh` ì‹¤í–‰ â†’ í™˜ê²½ êµ¬ì¶•
9. `python src/main_pipeline.py --input_video baseball.mp4 --visualize`

---

## ğŸ’¬ ë‹¤ìŒ ë‹¨ê³„ ì„ íƒì§€

**Q1. ì–´ë–¤ ê²ƒë¶€í„° ì‹œì‘í• ê¹Œìš”?**
- A. ìš°ì„  HMR ì¶”ë¡  í…ŒìŠ¤íŠ¸ (ë‹¨ì¼ ì´ë¯¸ì§€)
- B. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì½”ë“œ ë¨¼ì € ì‘ì„±
- C. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œë¶€í„°

**Q2. ì‚¬ìš© í™˜ê²½ì€?**
- A. ë¡œì»¬ PC (GPU ìˆìŒ)
- B. Google Colab
- C. í´ë¼ìš°ë“œ ì„œë²„

**Q3. ì¦‰ì‹œ í•„ìš”í•œ ì½”ë“œëŠ”?**
- A. ìœ„ì˜ ëª¨ë“  Python íŒŒì¼ì„ ë°”ë¡œ ìƒì„±
- B. íŠ¹ì • ëª¨ë“ˆë§Œ ë¨¼ì € (ì–´ë–¤ ê²ƒ?)
- C. ì „ì²´ í†µí•© íŒŒì¼ í•˜ë‚˜ë¡œ

ì„ íƒí•´ì£¼ì‹œë©´ í•´ë‹¹ ë¶€ë¶„ì„ ë°”ë¡œ êµ¬í˜„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤!
